version: '3.8'

services:
  # Redis Cache
  redis:
    image: redis:7.2-alpine
    container_name: verinode-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./config/redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
      - REDIS_MAXMEMORY=${REDIS_MAXMEMORY:-256mb}
      - REDIS_MAXMEMORY_POLICY=${REDIS_MAXMEMORY_POLICY:-allkeys-lru}
    restart: unless-stopped
    networks:
      - verinode-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Redis Commander - Redis management UI
  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: verinode-redis-commander
    ports:
      - "8081:8081"
    environment:
      - REDIS_HOSTS=local:redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
    restart: unless-stopped
    networks:
      - verinode-network
    depends_on:
      - redis

  # IPFS Node
  ipfs:
    image: ipfs/go-ipfs:latest
    container_name: verinode-ipfs
    ports:
      - "4001:4001"     # Swarm port
      - "4001:4001/udp" # Swarm UDP port
      - "5001:5001"     # API port
      - "8080:8080"     # Gateway port
    volumes:
      - ipfs_data:/data/ipfs
      - ./config/ipfs-config:/data/ipfs/config
    environment:
      - IPFS_PROFILE=server
      - IPFS_PATH=/data/ipfs
    command: >
      sh -c "
        ipfs config --json API.HTTPHeaders.Access-Control-Allow-Origin '[\"*\"]' &&
        ipfs config --json API.HTTPHeaders.Access-Control-Allow-Methods '[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\"]' &&
        ipfs config --json Gateway.HTTPHeaders.Access-Control-Allow-Origin '[\"*\"]' &&
        ipfs config --json Gateway.HTTPHeaders.Access-Control-Allow-Methods '[\"GET\", \"HEAD\", \"OPTIONS\"]' &&
        ipfs config --json Swarm.ConnMgr.HighWater 1000 &&
        ipfs config --json Swarm.ConnMgr.LowWater 100 &&
        ipfs config --json Datastore.StorageMax '\"10GB\"' &&
        ipfs daemon --migrate=true
      "
    restart: unless-stopped
    networks:
      - verinode-network
    healthcheck:
      test: ["CMD", "ipfs", "id"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Prometheus - Metrics collection and storage
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: verinode-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/config.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus/rules.yml:/etc/prometheus/rules/rules.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - monitoring
    depends_on:
      - ipfs

  # Grafana - Visualization and dashboards
  grafana:
    image: grafana/grafana:10.1.5
    container_name: verinode-grafana
    ports:
      - "3000:3000"
    volumes:
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_DOMAIN=localhost
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s:%(http_port)s/
    restart: unless-stopped
    networks:
      - monitoring
    depends_on:
      - prometheus
      - loki

  # Loki - Log aggregation
  loki:
    image: grafana/loki:2.9.2
    container_name: verinode-loki
    ports:
      - "3100:3100"
    volumes:
      - ./monitoring/loki/config.yml:/etc/loki/local-config.yaml
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    restart: unless-stopped
    networks:
      - monitoring

  # Promtail - Log shipper for Loki
  promtail:
    image: grafana/promtail:2.9.2
    container_name: verinode-promtail
    volumes:
      - ./logs:/var/log/verinode
      - ./monitoring/promtail/config.yml:/etc/promtail/config.yml
      - /var/log:/var/log
    command: -config.file=/etc/promtail/config.yml
    restart: unless-stopped
    networks:
      - monitoring
    depends_on:
      - loki

  # Alertmanager - Alert routing and notification
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: verinode-alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager/config.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    restart: unless-stopped
    networks:
      - monitoring
    depends_on:
      - prometheus

  # Node Exporter - System metrics
  node-exporter:
    image: prom/node-exporter:v1.6.1
    container_name: verinode-node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    restart: unless-stopped
    networks:
      - monitoring

  # MongoDB Exporter - Database metrics
  mongodb-exporter:
    image: percona/mongodb_exporter:0.39.0
    container_name: verinode-mongodb-exporter
    ports:
      - "9216:9216"
    environment:
      - MONGODB_URI=mongodb://mongodb:27017
    restart: unless-stopped
    networks:
      - monitoring
      - verinode-network
    depends_on:
      - mongodb

  # Blackbox Exporter - Uptime and endpoint monitoring
  blackbox-exporter:
    image: prom/blackbox-exporter:v0.24.0
    container_name: verinode-blackbox-exporter
    ports:
      - "9115:9115"
    volumes:
      - ./monitoring/blackbox/config.yml:/etc/blackbox-exporter/config.yml
    command:
      - '--config.file=/etc/blackbox-exporter/config.yml'
    restart: unless-stopped
    networks:
      - monitoring

  # Process Exporter - Process monitoring
  process-exporter:
    image: ncabatoff/process-exporter:0.7.10
    container_name: verinode-process-exporter
    ports:
      - "9256:9256"
    volumes:
      - /proc:/host/proc:ro
    command:
      - '--procfs=/host/proc'
      - '--config.path=/etc/process-exporter/config.yml'
    restart: unless-stopped
    networks:
      - monitoring

  # Verinode Backend with monitoring and IPFS
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: verinode-backend
    ports:
      - "3001:3001"
    environment:
      - NODE_ENV=production
      - PORT=3001
      - MONGODB_URI=mongodb://mongodb:27017/verinode
      - PROMETHEUS_ENDPOINT=http://prometheus:9090
      - LOKI_ENDPOINT=http://loki:3100
      - IPFS_HOST=ipfs
      - IPFS_PORT=5001
      - IPFS_PROTOCOL=http
      - IPFS_GATEWAY_PORT=8080
      - IPFS_GATEWAY_HOST=0.0.0.0
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
    volumes:
      - ./backend/logs:/app/logs
      - ./monitoring:/app/monitoring
      - ./config:/app/config
    restart: unless-stopped
    networks:
      - verinode-network
      - monitoring
    depends_on:
      - mongodb
      - prometheus
      - loki
      - ipfs
      - redis

  # MongoDB
  mongodb:
    image: mongo:7.0
    container_name: verinode-mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
      - ./scripts/mongodb-init.js:/docker-entrypoint-initdb.d/mongodb-init.js:ro
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_ROOT_USER:-admin}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_ROOT_PASSWORD:-password}
      - MONGO_INITDB_DATABASE=verinode
    restart: unless-stopped
    networks:
      - verinode-network

  # Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: verinode-frontend
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:3001/api
      - REACT_APP_MONITORING_URL=http://localhost:3000
      - REACT_APP_IPFS_GATEWAY_URL=http://localhost:8080
    restart: unless-stopped
    networks:
      - verinode-network
    depends_on:
      - backend
      - ipfs

  # Contract Monitor
  contract-monitor:
    build:
      context: .
      dockerfile: Dockerfile.contract-monitor
    container_name: verinode-contract-monitor
    volumes:
      - ./scripts:/app/scripts
      - ./monitoring:/app/monitoring
      - ./logs:/app/logs
    environment:
      - STELLAR_NETWORK=testnet
      - HORIZON_URL=https://horizon-testnet.stellar.org
      - MONITORING_INTERVAL=5000
      - IPFS_HOST=ipfs
      - IPFS_PORT=5001
    restart: unless-stopped
    networks:
      - monitoring
      - verinode-network
    depends_on:
      - prometheus
      - loki
      - ipfs

  # IPFS Cluster (for advanced pinning and replication)
  ipfs-cluster:
    image: ipfs/ipfs-cluster:latest
    container_name: verinode-ipfs-cluster
    ports:
      - "9094:9094"   # Cluster API
      - "9095:9095"   # Cluster proxy
    volumes:
      - ipfs_cluster_data:/data/ipfs-cluster
      - ./config/ipfs-cluster:/data/ipfs-cluster/config
    environment:
      - CLUSTER_SECRET=${IPFS_CLUSTER_SECRET:-}
      - CLUSTER_PEERNAME=verinode-cluster
    command: >
      sh -c "
        ipfs-cluster-service init &&
        ipfs-cluster-service daemon
      "
    restart: unless-stopped
    networks:
      - verinode-network
    depends_on:
      - ipfs

volumes:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  loki_data:
    driver: local
  alertmanager_data:
    driver: local
  mongodb_data:
    driver: local
  redis_data:
    driver: local
  ipfs_data:
    driver: local
  ipfs_cluster_data:
    driver: local

networks:
  monitoring:
    driver: bridge
  verinode-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
